{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DLP Lab5\n",
    "Goal of this lab is to to implement a conditional seq2seq VAE for English tense conversion\n",
    "1. Tense conversion: ‘access’ to ‘accessing’, or ‘accessed’ to ‘accesses’\n",
    "2. Generative model: Gaussian noise + tense -> access, accesses, accessing, accessed\n",
    "\n",
    "#### Requirment\n",
    "1. Implement a conditional seq2seq VAE.\n",
    "    * Modify encoder, decoder, and training functions\n",
    "    * Implement evaluation function, dataloader, and reparameterization trick.\n",
    "2. Plot and **compare** the CrossEntropy loss, KL loss and BLEU-4 score of testing data curves during training with different settings of your model\n",
    "    * Teacher forcing ratio\n",
    "    * KL annealing schedules (two methods)\n",
    "3. Output the conversion results between tenses (from tense A to tense B)\n",
    "4. Output the results generated by a Gaussian noise with 4 tenses.\n",
    "\n",
    "#### Implement detail\n",
    "1. Use LSTM\n",
    "2. Log variance\n",
    "3. Condition (tense)\n",
    "    * Simply concatenate to the hidden_0 and z\n",
    "    * Embed your condition to high dimensional space (or simply use one-hot)\n",
    "4. KL lost annealing\n",
    "    * Monotonic\n",
    "    * Cyclical\n",
    "5. Adopt BLEU-4 score function in NLTK (average 10 testing scores)\n",
    "6. Adopt Gaussian_score() to compute the generation score\n",
    "    * Random sample 100 noise to generate 100 words with 4 different tenses(totally 400 words)\n",
    "    * 4 words should exactly match the training data\n",
    "\n",
    "#### Hyper parameters\n",
    "* LSTM hidden size: 256 or 512\n",
    "* Latent size: 32\n",
    "* Condition embedding size: 8\n",
    "* Teacher forcing ratio: 0~1 (>0.5) (??? \n",
    "* KL weight: 0~1 (???\n",
    "* Learning rate: 0.05\n",
    "* Optimizer: SGD\n",
    "* Loss function: torch.nn.CrossEntropyLoss()\n",
    "\n",
    "Date: 2020/05/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.142035Z",
     "start_time": "2020-05-12T06:21:02.385112Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import string\n",
    "import re\n",
    "import gc\n",
    "import random\n",
    "import time\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils import data\n",
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import system\n",
    "from nltk.translate.bleu_score import SmoothingFunction, sentence_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.167563Z",
     "start_time": "2020-05-12T06:21:03.143236Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "#----------Hyper Parameters----------#\n",
    "hidden_size = 256\n",
    "laten_size = 32\n",
    "condition_size = 4\n",
    "#The number of vocabulary\n",
    "vocab_size = 30\n",
    "teacher_forcing_ratio = 0.7\n",
    "empty_input_ratio = 0.1\n",
    "KLD_weight = 0.0\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.171874Z",
     "start_time": "2020-05-12T06:21:03.168643Z"
    }
   },
   "outputs": [],
   "source": [
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch data\n",
    "* train.txt: Each training pair includes 4 words: simple present(sp), third person(tp), present progressive(pg), simple past(p)\n",
    "* test.txt: Each training pair includes 2 words with different combination of tenses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.187383Z",
     "start_time": "2020-05-12T06:21:03.172809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>abandon</td>\n",
       "      <td>abandons</td>\n",
       "      <td>abandoning</td>\n",
       "      <td>abandoned</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>abet</td>\n",
       "      <td>abets</td>\n",
       "      <td>abetting</td>\n",
       "      <td>abetted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>abdicate</td>\n",
       "      <td>abdicates</td>\n",
       "      <td>abdicating</td>\n",
       "      <td>abdicated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>abduct</td>\n",
       "      <td>abducts</td>\n",
       "      <td>abducting</td>\n",
       "      <td>abducted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>abound</td>\n",
       "      <td>abounds</td>\n",
       "      <td>abounding</td>\n",
       "      <td>abounded</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1222</th>\n",
       "      <td>exhort</td>\n",
       "      <td>exhorts</td>\n",
       "      <td>exhorting</td>\n",
       "      <td>exhorted</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1223</th>\n",
       "      <td>exhilarate</td>\n",
       "      <td>exhilarates</td>\n",
       "      <td>exhilarating</td>\n",
       "      <td>exhilarated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1224</th>\n",
       "      <td>exculpate</td>\n",
       "      <td>exculpates</td>\n",
       "      <td>exculpating</td>\n",
       "      <td>exculpated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1225</th>\n",
       "      <td>exasperate</td>\n",
       "      <td>exasperates</td>\n",
       "      <td>exasperating</td>\n",
       "      <td>exasperated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1226</th>\n",
       "      <td>exacerbate</td>\n",
       "      <td>exacerbates</td>\n",
       "      <td>exacerbating</td>\n",
       "      <td>exacerbated</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1227 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0            1             2            3\n",
       "0        abandon     abandons    abandoning    abandoned\n",
       "1           abet        abets      abetting      abetted\n",
       "2       abdicate    abdicates    abdicating    abdicated\n",
       "3         abduct      abducts     abducting     abducted\n",
       "4         abound      abounds     abounding     abounded\n",
       "...          ...          ...           ...          ...\n",
       "1222      exhort      exhorts     exhorting     exhorted\n",
       "1223  exhilarate  exhilarates  exhilarating  exhilarated\n",
       "1224   exculpate   exculpates   exculpating   exculpated\n",
       "1225  exasperate  exasperates  exasperating  exasperated\n",
       "1226  exacerbate  exacerbates  exacerbating  exacerbated\n",
       "\n",
       "[1227 rows x 4 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getData(mode):\n",
    "    assert mode == 'train' or mode == 'test'\n",
    "    if mode == 'train':\n",
    "        data = pd.read_csv('./data/'+mode+'.txt', delimiter=' ', header=None)\n",
    "    else:\n",
    "        data = []\n",
    "        with open('./data/test.txt','r') as fp:\n",
    "            for line in fp:\n",
    "                word = line.split(' ')\n",
    "                word[1] = word[1].strip('\\n')\n",
    "                data.extend([word])\n",
    "    return data\n",
    "\n",
    "getData(\"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.194579Z",
     "start_time": "2020-05-12T06:21:03.188387Z"
    }
   },
   "outputs": [],
   "source": [
    "class Vocabuary():\n",
    "    def __init__(self):\n",
    "        self.word2index = {'SOS': 0, 'EOS': 1, 'PAD': 2, 'UNK': 3}\n",
    "        self.index2word = {0: 'SOS', 1: 'EOS', 2: 'PAD', 3: 'UNK'}\n",
    "        self.n_words = 4\n",
    "        self.max_length = 0\n",
    "        self.build_vocab(getData('train'))\n",
    "        \n",
    "\n",
    "    # input the training data and build vocabulary\n",
    "    def build_vocab(self, corpus):        \n",
    "        for idx in range(corpus.shape[0]):\n",
    "            for word in corpus.iloc[idx,:]:\n",
    "                if len(word) > self.max_length:\n",
    "                    self.max_length = len(word)\n",
    "                    \n",
    "                for char in word:\n",
    "                    if char not in self.word2index:\n",
    "                        self.word2index[char] = self.n_words\n",
    "                        self.index2word[self.n_words] = char\n",
    "                        self.n_words += 1                      \n",
    "                    \n",
    "    # convert word in indices\n",
    "    def word2indices(self, word, add_eos=False, add_sos=False):\n",
    "        indices = [self.word2index[char] if char in self.word2index else 3 for char in word]\n",
    "\n",
    "        if add_sos:\n",
    "            indices.insert(0, 0)\n",
    "        if add_eos:\n",
    "            indices.append(1)\n",
    "            \n",
    "        # padding input of same target into same length\n",
    "#         indices.extend([2]*(self.max_length-len(word)))\n",
    "            \n",
    "        return np.array(indices)\n",
    "    \n",
    "    # convert indices to word\n",
    "    def indices2word(self, indices):\n",
    "        # ignore indices after EOS\n",
    "        new_indices = []\n",
    "        for idx in indices:\n",
    "            new_indices.append(idx)\n",
    "            if idx == self.word2index['EOS']:\n",
    "                break\n",
    "                    \n",
    "        word = [self.index2word[idx] for idx in new_indices if idx > 2 ]\n",
    "        return ''.join(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.335211Z",
     "start_time": "2020-05-12T06:21:03.195530Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12 29 14 15 20 17  4 13 12  7]\n",
      "exculpated\n"
     ]
    }
   ],
   "source": [
    "v = Vocabuary()\n",
    "t = \"exculpated\"\n",
    "idx = v.word2indices(t)\n",
    "print(idx)\n",
    "t = v.indices2word(idx)\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.342195Z",
     "start_time": "2020-05-12T06:21:03.336704Z"
    }
   },
   "outputs": [],
   "source": [
    "class TenseLoader(data.Dataset):\n",
    "    def __init__(self, mode, vocab):\n",
    "        self.mode = mode   \n",
    "        self.data = getData(self.mode)\n",
    "        self.vocab = vocab\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        data_tensor = []\n",
    "        if self.mode == 'train':\n",
    "            for word in self.data.iloc[index,:]:\n",
    "                data_tensor.append(torch.tensor(self.vocab.word2indices(word)))\n",
    "#             data_tensor = torch.tensor(data_tensor)\n",
    "        else:\n",
    "            condition = [[\"sp\", \"p\"], [\"sp\", \"pg\"], [\"sp\", \"tp\"], [\"sp\", \"tp\"], [\"p\", \"tp\"], \n",
    "                        [\"sp\", \"pg\"], [\"p\", \"sp\"], [\"pg\", \"sp\"], [\"pg\", \"p\"], [\"pg\", \"tp\"]]\n",
    "            order = {'sp':0, 'tp':1, 'pg':2, 'p':3}\n",
    "            \n",
    "            input_tense = order[condition[index][0]]\n",
    "            input_tensor = torch.tensor(self.vocab.word2indices(self.data[index][0]))\n",
    "            target_tense = order[condition[index][1]]\n",
    "            target_tensor = torch.tensor(self.vocab.word2indices(self.data[index][1]))\n",
    "            \n",
    "            data_tensor = [(input_tense, input_tensor), (target_tense, target_tensor)]\n",
    "\n",
    "        return data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.352142Z",
     "start_time": "2020-05-12T06:21:03.343343Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 4, 14, 21, 10, 12, 22, 12]),\n",
       " tensor([ 4, 14, 21, 10, 12, 22, 12,  9]),\n",
       " tensor([ 4, 14, 21, 10, 12, 22, 10,  6, 11]),\n",
       " tensor([ 4, 14, 21, 10, 12, 22, 12,  7])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset = TenseLoader('train', v)\n",
    "trainset[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.356277Z",
     "start_time": "2020-05-12T06:21:03.353054Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, tensor([4, 5, 4, 6, 7, 8, 6])),\n",
       " (3, tensor([ 4,  5,  4,  6,  7,  8,  6, 12,  7]))]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testset = TenseLoader('test', v)\n",
    "testset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Show reslut\n",
    "* Crossentropy loss curve\n",
    "* KL loss curve\n",
    "* BLEU-4 score curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.362113Z",
     "start_time": "2020-05-12T06:21:03.357369Z"
    }
   },
   "outputs": [],
   "source": [
    "def show_result(scores, losses):  \n",
    "    bleu_score, gaussian_score = scores\n",
    "    c_loss, kl_loss = losses\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"CrossEntropy Loss Curve\", fontsize=18)\n",
    "    plt.plot(c_loss)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"KL Loss Curve\", fontsize=18)\n",
    "    plt.plot(kl_loss)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"BLEU Score Curve\", fontsize=18)\n",
    "    plt.plot(bleu_score)\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.ylabel(\"Score\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.title(\"Gaussian Score Curve\", fontsize=18)\n",
    "    plt.plot(gaussian_score)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition\n",
    "concatenate the condition part with the initial hidden part \n",
    "* nn.Embedding\n",
    "* One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.372166Z",
     "start_time": "2020-05-12T06:21:03.363116Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([[1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.],\n",
       "         [1., 0., 0., 0.]]), tensor([[0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.],\n",
       "         [0., 1., 0., 0.]]), tensor([[0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.],\n",
       "         [0., 0., 1., 0.]]), tensor([[0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.],\n",
       "         [0., 0., 0., 1.]])]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def condition_embedding(condition_size, batch_size=1, condition=None):\n",
    "    order = {'sp':0, 'tp':1, 'pg':2, 'p':3}\n",
    "    one_hot = {0:[1, 0, 0, 0], 1:[0, 1, 0, 0], 2:[0, 0, 1, 0], 3:[0, 0, 0, 1]}\n",
    "    \n",
    "    if condition != None:\n",
    "        embedded_tense = torch.tensor(one_hot[condition], dtype=torch.float).view(1, -1)\n",
    "    else: \n",
    "        embedded_tense = []\n",
    "        for o in order.values():\n",
    "            embedded = torch.tensor(one_hot[o], dtype=torch.float).view(1, -1)\n",
    "            embedded_cp = embedded\n",
    "            \n",
    "            # expand batch dim, (4, condition_size) to (4, batch_size, condition_size)\n",
    "            for i in range(batch_size-1):\n",
    "                embedded = torch.cat((embedded, embedded_cp), 0)\n",
    "            embedded_tense.append(embedded)\n",
    "        \n",
    "    return embedded_tense\n",
    "                     \n",
    "condition_embedding(condition_size, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.375770Z",
     "start_time": "2020-05-12T06:21:03.373051Z"
    }
   },
   "outputs": [],
   "source": [
    "#compute BLEU-4 score\n",
    "def compute_bleu(output, reference):\n",
    "    \"\"\"\n",
    "    reference = 'accessed'\n",
    "    output = 'access'\n",
    "    return BLEU score\n",
    "    \"\"\"\n",
    "    cc = SmoothingFunction()\n",
    "    if len(reference) == 3:\n",
    "        weights = (0.33,0.33,0.33)\n",
    "    else:\n",
    "        weights = (0.25,0.25,0.25,0.25)\n",
    "    return sentence_bleu([reference], output,weights=weights,smoothing_function=cc.method1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.383432Z",
     "start_time": "2020-05-12T06:21:03.376652Z"
    }
   },
   "outputs": [],
   "source": [
    "def BLEU_predict(encoder, decoder, vocab, batch_size=1, condition_size=8, plot_pred=False):\n",
    "    testset = TenseLoader('test', vocab)\n",
    "    outputs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for idx in range(len(testset)):\n",
    "            input_tense = testset[idx][0][0]\n",
    "            input_embedded_tense = condition_embedding(condition_size, condition=input_tense) # (1, condition_szie)\n",
    "            input_tensor = testset[idx][0][1].to(device) # (seq_len)\n",
    "\n",
    "            target_tense = testset[idx][1][0]\n",
    "            target_embedded_tense = condition_embedding(condition_size, condition=target_tense)\n",
    "            target_tensor = testset[idx][1][1].to(device)\n",
    "\n",
    "            batch_size = 1\n",
    "\n",
    "            # transpose tensor from (batch_size, tense, seq_len) to (tense, seq_len, batch_size)\n",
    "            input_tensor = input_tensor.view(-1, 1) # (seq_len, 1)\n",
    "            target_tensor = target_tensor.view(-1, 1)\n",
    "\n",
    "            # init encoder hidden state and cat condition\n",
    "            encoder_hidden = encoder.initHidden(input_embedded_tense, batch_size)\n",
    "\n",
    "            # calculate number of time step\n",
    "            input_length = input_tensor.size(0)\n",
    "            target_length = target_tensor.size(0)\n",
    "            \n",
    "\n",
    "            #----------sequence to sequence part for encoder----------#\n",
    "            for ei in range(input_length):\n",
    "                encoder_output, encoder_hidden = encoder(\n",
    "                    input_tensor[ei], encoder_hidden)\n",
    "\n",
    "            # reparameterization trick\n",
    "            mu, logvar = encoder.variational(encoder_hidden)\n",
    "            reparameterized_state = reparameterize(mu, logvar)\n",
    "            reparameterized_state = decoder.in_layer(reparameterized_state)\n",
    "\n",
    "            # init decoder hidden state and cat condition\n",
    "            decoder_hidden = decoder.initHidden(reparameterized_state, target_embedded_tense, batch_size)\n",
    "            \n",
    "            decoder_input = torch.tensor([[SOS_token] for i in range(batch_size)], device=device)\n",
    "            \n",
    "            output = torch.zeros(target_length, batch_size)\n",
    "\n",
    "            #----------sequence to sequence part for decoder----------#\n",
    "            for di in range(target_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                output[di] = decoder_input\n",
    "            \n",
    "            # transpose tensor from (target_length, batch_size) to (batch_size, target_length)\n",
    "            output = output.transpose(0, 1).view(-1)\n",
    "         \n",
    "            outputs.append(vocab.indices2word(output.data.numpy()))\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.387320Z",
     "start_time": "2020-05-12T06:21:03.384303Z"
    }
   },
   "outputs": [],
   "source": [
    "# print the prediction and return the bleu score\n",
    "def BLEU_score(prediction, plot_pred=False):\n",
    "    data = getData('test')\n",
    "\n",
    "    bleu_total = 0\n",
    "    for idx in range(len(prediction)):\n",
    "        bleu_total += compute_bleu(prediction[idx], data[idx][1])\n",
    "\n",
    "        if plot_pred:\n",
    "            output = \"\\ninput:  {}\\ntarget: {}\\npred:   {}\".format(data[idx][0], data[idx][1], prediction[idx])\n",
    "            print (\"=\"*30+output)\n",
    "\n",
    "    return bleu_total/len(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.393806Z",
     "start_time": "2020-05-12T06:21:03.388297Z"
    }
   },
   "outputs": [],
   "source": [
    "def Gaussian_predict(encoder, decoder, vocab, batch_size=64, laten_size=32, condition_size=8, plot_pred=False):\n",
    "    outputs = []\n",
    "\n",
    "    with torch.no_grad():    \n",
    "        batch_size = 100\n",
    "\n",
    "        # sample 100 Gaussian\n",
    "        laten_variable = torch.randn((batch_size, laten_size), device=device).view(1, batch_size, -1)\n",
    "\n",
    "        # get 4 tense embedding tensor\n",
    "        embedded_tenses = condition_embedding(condition_size , batch_size)\n",
    "\n",
    "        # record outputs\n",
    "        output_tensors = torch.zeros(4, vocab.max_length, batch_size)# (tense, seq_len, batch_size)\n",
    "\n",
    "        # 4 tense iteration\n",
    "        for index, embedded_tense in enumerate(embedded_tenses):\n",
    "\n",
    "            decoder_input = torch.tensor([[SOS_token] for i in range(batch_size)], device=device)\n",
    "\n",
    "            output = torch.zeros(vocab.max_length, batch_size)\n",
    "\n",
    "            # init decoder hidden state and cat condition\n",
    "            decoder_hidden = decoder.initHidden(decoder.in_layer(laten_variable), embedded_tense, batch_size)\n",
    "\n",
    "            #----------sequence to sequence part for decoder----------#\n",
    "            for di in range(vocab.max_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "                output[di] = decoder_input\n",
    "\n",
    "            # get predict tensors\n",
    "            output_tensors[index] = output\n",
    "\n",
    "        # transpose tensor from (tense, seq_len, batch_size) to (batch_size, tense, seq_len)\n",
    "        output_tensors = output_tensors.permute(2, 0, 1)\n",
    "\n",
    "        # convert input into string\n",
    "        for idx in range(batch_size):\n",
    "            outputs.append([vocab.indices2word(tense.data.numpy()) for tense in output_tensors[idx]])\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.398190Z",
     "start_time": "2020-05-12T06:21:03.394719Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# compute generation score\n",
    "def Gaussian_score(predictions, plot_pred=False):\n",
    "    \"\"\"\n",
    "    the order should be : simple present, third person, present progressive, past\n",
    "    predictions = [['consult', 'consults', 'consulting', 'consulted'],...]\n",
    "    return Gaussian_score score\n",
    "    \"\"\"\n",
    "    score = 0\n",
    "    words_list = []\n",
    "    with open('./data/train.txt','r') as fp:\n",
    "        for line in fp:\n",
    "            word = line.split(' ')\n",
    "            word[3] = word[3].strip('\\n')\n",
    "            words_list.extend([word])\n",
    "        for idx, t in enumerate(predictions):\n",
    "            if plot_pred:\n",
    "                print (t)\n",
    "            for idxj, i in enumerate(words_list):\n",
    "                if t == i:\n",
    "                    score += 1\n",
    "    return score/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.402513Z",
     "start_time": "2020-05-12T06:21:03.399152Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, vocab, batch_size=64, laten_size=32, condition_size=8, plot_pred=False):\n",
    "    # predict train.txt for gaussian score\n",
    "    predictions = Gaussian_predict(encoder, decoder, vocab, batch_size=batch_size, laten_size=laten_size, condition_size=condition_size, plot_pred=plot_pred)\n",
    "    \n",
    "    # compute Gaussian score\n",
    "    gaussian_score = Gaussian_score(predictions, plot_pred=plot_pred)\n",
    "    if plot_pred:\n",
    "        print (\"Gaussian score: %.2f\"%gaussian_score)\n",
    "\n",
    "    # predict test.txt for bleu score\n",
    "    predictions = BLEU_predict(encoder, decoder, vocab, batch_size=1, condition_size=condition_size, plot_pred=False)\n",
    "            \n",
    "    # compute BLEU score\n",
    "    bleu_score = BLEU_score(predictions, plot_pred=plot_pred)\n",
    "    if plot_pred:\n",
    "        print (\"BLEU score: %.2f\"%bleu_score)\n",
    "    \n",
    "    return bleu_score, gaussian_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reparameterization Trick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.405586Z",
     "start_time": "2020-05-12T06:21:03.403406Z"
    }
   },
   "outputs": [],
   "source": [
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5*logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps*std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.411576Z",
     "start_time": "2020-05-12T06:21:03.406451Z"
    }
   },
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, laten_size, condition_size):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size+condition_size, laten_size)\n",
    "        self.fc2 = nn.Linear(hidden_size+condition_size, laten_size)\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size+condition_size)\n",
    "        self.lstm = nn.LSTM(hidden_size+condition_size, hidden_size+condition_size)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input).view(1, -1, self.hidden_size+self.condition_size)\n",
    "        output = embedded\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        return output, hidden\n",
    "    \n",
    "    def variational(self, hidden):\n",
    "        return self.fc1(hidden[0]), self.fc2(hidden[0])\n",
    "\n",
    "    def initHidden(self, embedded_tense, batch_size=64):\n",
    "        embedded_tense = embedded_tense.to(device).view(1, batch_size, -1)\n",
    "        zeros = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return (torch.cat((zeros, embedded_tense), 2),\n",
    "                torch.cat((zeros, embedded_tense), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.417610Z",
     "start_time": "2020-05-12T06:21:03.412426Z"
    }
   },
   "outputs": [],
   "source": [
    "class DecoderRNN(nn.Module):\n",
    "    def __init__(self, latent_size, hidden_size, output_size, condition_size):\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.condition_size = condition_size\n",
    "\n",
    "        self.in_layer = nn.Linear(latent_size, hidden_size)\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size+condition_size)\n",
    "        self.lstm = nn.LSTM(hidden_size+condition_size, hidden_size+condition_size)\n",
    "        self.out = nn.Linear(hidden_size+condition_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        output = self.embedding(input).view(1, -1, self.hidden_size+self.condition_size)\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.lstm(output, hidden)\n",
    "        output = self.out(output[0])\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self, hidden_state, embedded_tense, batch_size):\n",
    "        #print(hidden_state.shape)\n",
    "        embedded_tense = embedded_tense.to(device).view(1, batch_size, -1)\n",
    "        zeros = torch.zeros(1, batch_size, self.hidden_size, device=device)\n",
    "        return (torch.cat((hidden_state, embedded_tense), 2),\n",
    "                torch.cat((zeros, embedded_tense), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n",
    "* Use teacher forcing\n",
    "* Use KL loss annealing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.421507Z",
     "start_time": "2020-05-12T06:21:03.418578Z"
    }
   },
   "outputs": [],
   "source": [
    "def kl_annealing(epochs, mode):\n",
    "    assert mode == \"monotonic\" or mode == \"cyclical\"\n",
    "    if mode == \"monotonic\":\n",
    "        if epochs > 20:\n",
    "            KLD_weight = 1\n",
    "        else:\n",
    "            KLD_weight = 0.05 * epochs\n",
    "    else:\n",
    "        if epochs%20 > 10:\n",
    "            KLD_weight = 1\n",
    "        else:\n",
    "            KLD_weight = 0.1 * (epochs%20)\n",
    "    return KLD_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.425122Z",
     "start_time": "2020-05-12T06:21:03.423026Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_kl_loss(mu, logvar):\n",
    "    # 0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.433693Z",
     "start_time": "2020-05-12T06:21:03.426338Z"
    }
   },
   "outputs": [],
   "source": [
    "# save model every epoch\n",
    "def train(input_tensors, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "          criterion, epochs, condition_size, teacher_forcing_ratio):\n",
    "#     batch_size = input_tensors.size(0)\n",
    "    batch_size = 1\n",
    "    # get 4 tense embedding tensor\n",
    "    embedded_tenses = condition_embedding(condition_size, batch_size)\n",
    "    \n",
    "    # loss for 4 tense\n",
    "    kl_loss_total = 0\n",
    "    ce_loss_total = 0\n",
    "    \n",
    "    # transpose tensor from (batch_size, tense, seq_len) to (tense, seq_len, batch_size)\n",
    "#     input_tensors = input_tensors.permute(1, 2, 0)\n",
    "    \n",
    "    # 4 tense iteration\n",
    "    for index, embedded_tense in enumerate(embedded_tenses):\n",
    "        # embedded_tense.to(device)\n",
    "        input_tensor = input_tensors[index].to(device) # (seq_len, batch_size)\n",
    "        input_tensor = input_tensor.transpose(0, 1)\n",
    "    \n",
    "        # init encoder hidden state and cat condition\n",
    "        encoder_hidden = encoder.initHidden(embedded_tense, batch_size)\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        # calculate number of time step\n",
    "        input_length = input_tensor.size(0)\n",
    "\n",
    "        loss = 0\n",
    "        ce_loss = 0\n",
    "\n",
    "        #----------sequence to sequence part for encoder----------#\n",
    "        for ei in range(input_length):\n",
    "            encoder_output, encoder_hidden = encoder(\n",
    "                input_tensor[ei], encoder_hidden)\n",
    "            \n",
    "        # reparameterization trick\n",
    "        mu, logvar = encoder.variational(encoder_hidden)\n",
    "        reparameterized_state = reparameterize(mu, logvar)\n",
    "        reparameterized_state = decoder.in_layer(reparameterized_state)\n",
    "        # calculate kl loss\n",
    "        kl_loss = compute_kl_loss(mu, logvar)\n",
    "        kl_loss_total += kl_loss\n",
    "        loss += kl_annealing(epochs, \"monotonic\") * kl_loss\n",
    "        \n",
    "        # init decoder hidden state and cat condition\n",
    "        decoder_hidden = decoder.initHidden(reparameterized_state, embedded_tense, batch_size)\n",
    "        \n",
    "        decoder_input = torch.tensor([[SOS_token] for i in range(batch_size)], device=device)\n",
    "\n",
    "        use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "        #----------sequence to sequence part for decoder----------#\n",
    "        if use_teacher_forcing:\n",
    "            # Teacher forcing: Feed the target as the next input\n",
    "            for di in range(input_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "                _, indx = torch.max(decoder_output, 1)\n",
    "#                 print(indx, input_tensor[di])\n",
    "                ce_loss += criterion(decoder_output, input_tensor[di])\n",
    "                decoder_input = input_tensor[di]  # Teacher forcing\n",
    "                \n",
    "        else:\n",
    "            # Without teacher forcing: use its own predictions as the next input\n",
    "            for di in range(input_length):\n",
    "                decoder_output, decoder_hidden = decoder(\n",
    "                    decoder_input, decoder_hidden) \n",
    "                topv, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze().detach()  # detach from history as input\n",
    "\n",
    "                ce_loss += criterion(decoder_output, input_tensor[di])\n",
    "#                 if decoder_input.item() == EOS_token:\n",
    "#                     break\n",
    "\n",
    "        loss += ce_loss\n",
    "        ce_loss_total += (ce_loss.item()/input_length)\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "    return ce_loss_total/4, kl_loss_total/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-12T06:21:03.441901Z",
     "start_time": "2020-05-12T06:21:03.434604Z"
    }
   },
   "outputs": [],
   "source": [
    "def trainIters(encoder, decoder, vocab, n_iters, print_every=1000, plot_every=100, \n",
    "               batch_size=64, learning_rate=0.01, laten_size=32, condition_size=8, teacher_forcing_ratio=1.0):\n",
    "    start = time.time()\n",
    "\n",
    "    # Reset every print_every, for print log\n",
    "    print_ce_loss_total = 0  \n",
    "    print_kl_loss_total = 0\n",
    "    # Reset every plot_every, for plot curve\n",
    "    crossentropy_losses = []\n",
    "    kl_losses = []\n",
    "    plot_ce_loss_total = 0\n",
    "    plot_kl_loss_total = 0\n",
    "    # scores\n",
    "    gaussian_scores = []\n",
    "    bleu_scores = []\n",
    "    \n",
    "\n",
    "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
    "\n",
    "    # create dataloader\n",
    "    trainset = TenseLoader('train', vocab)\n",
    "    trainloader = data.DataLoader(trainset, batch_size, shuffle = True)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for iter in range(1, n_iters + 1):\n",
    "        for input_tensors in trainloader:\n",
    "#             input_tensors = input_tensors.to(device)\n",
    "\n",
    "            ce_loss, kl_loss = train(input_tensors, encoder, decoder, encoder_optimizer, decoder_optimizer, \n",
    "                                     criterion, (iter-1), condition_size, teacher_forcing_ratio)\n",
    "            print_ce_loss_total += ce_loss\n",
    "            print_kl_loss_total += kl_loss\n",
    "            plot_ce_loss_total += ce_loss\n",
    "            plot_kl_loss_total += kl_loss\n",
    "            \n",
    "        # evaluate and save model\n",
    "        bleu_score, gaussian_score = evaluate(encoder, decoder, vocab, laten_size=laten_size, condition_size=condition_size, plot_pred=False)\n",
    "        bleu_scores.append(bleu_score)\n",
    "        gaussian_scores.append(gaussian_score)\n",
    "        if bleu_score > 0.1 and gaussian_score > 0.2:\n",
    "            print (\"Model save...\")\n",
    "            torch.save(encoder, \"./models/encoder_{:.4f}_{:.4f}.ckpt\".format(gaussian_score, bleu_score))\n",
    "            torch.save(decoder, \"./models/decoder_{:.4f}_{:.4f}.ckpt\".format(gaussian_score, bleu_score))\n",
    "\n",
    "        if iter % print_every == 0:\n",
    "            print_ce_loss_avg = print_ce_loss_total / print_every\n",
    "            print_kl_loss_avg = print_kl_loss_total / print_every\n",
    "            print('%s (%d %d%%) CE Loss: %.4f, KL Loss: %.4f, BLEU score: %.2f, Gaussian score: %.2f' % \n",
    "                  (timeSince(start, iter / n_iters), iter, iter / n_iters * 100, print_ce_loss_avg, print_kl_loss_avg, bleu_score, gaussian_score))\n",
    "            print_ce_loss_total = 0\n",
    "            print_kl_loss_total = 0\n",
    "\n",
    "        crossentropy_losses.append(plot_ce_loss_total)\n",
    "        plot_ce_loss_total = 0\n",
    "        kl_losses.append(plot_kl_loss_total)\n",
    "        plot_kl_loss_total = 0\n",
    "        \n",
    "        collected = gc.collect()\n",
    "            \n",
    "#     print (\"The highest score is %s\"%max_score)\n",
    "            \n",
    "    return (bleu_scores, gaussian_scores), (crossentropy_losses, kl_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-12T06:21:02.406Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0m 41s (- 68m 42s) (1 1%) CE Loss: 3004.9755, KL Loss: 1373.0375, BLEU score: 0.05, Gaussian score: 0.00\n",
      "1m 23s (- 68m 29s) (2 2%) CE Loss: 2752.4332, KL Loss: 2109.3921, BLEU score: 0.08, Gaussian score: 0.00\n",
      "2m 8s (- 69m 8s) (3 3%) CE Loss: 2617.9485, KL Loss: 2623.5410, BLEU score: 0.08, Gaussian score: 0.00\n",
      "2m 53s (- 69m 27s) (4 4%) CE Loss: 2508.6193, KL Loss: 2831.7866, BLEU score: 0.08, Gaussian score: 0.00\n",
      "3m 37s (- 68m 57s) (5 5%) CE Loss: 2390.9205, KL Loss: 3479.9385, BLEU score: 0.08, Gaussian score: 0.00\n",
      "4m 22s (- 68m 28s) (6 6%) CE Loss: 2255.9658, KL Loss: 3813.6475, BLEU score: 0.09, Gaussian score: 0.00\n",
      "5m 6s (- 67m 49s) (7 7%) CE Loss: 2155.1569, KL Loss: 4249.3066, BLEU score: 0.09, Gaussian score: 0.00\n",
      "5m 50s (- 67m 15s) (8 8%) CE Loss: 2115.7320, KL Loss: 4260.1543, BLEU score: 0.11, Gaussian score: 0.00\n",
      "6m 35s (- 66m 40s) (9 9%) CE Loss: 2063.5628, KL Loss: 4219.1533, BLEU score: 0.11, Gaussian score: 0.00\n",
      "7m 20s (- 66m 0s) (10 10%) CE Loss: 1987.6929, KL Loss: 4164.6060, BLEU score: 0.14, Gaussian score: 0.00\n",
      "8m 5s (- 65m 25s) (11 11%) CE Loss: 1994.4886, KL Loss: 3991.8965, BLEU score: 0.08, Gaussian score: 0.00\n",
      "8m 49s (- 64m 43s) (12 12%) CE Loss: 1999.4210, KL Loss: 3778.8513, BLEU score: 0.08, Gaussian score: 0.00\n",
      "9m 34s (- 64m 3s) (13 13%) CE Loss: 1971.1662, KL Loss: 3510.2742, BLEU score: 0.10, Gaussian score: 0.00\n",
      "10m 18s (- 63m 19s) (14 14%) CE Loss: 2021.3152, KL Loss: 3259.7090, BLEU score: 0.08, Gaussian score: 0.00\n",
      "11m 3s (- 62m 40s) (15 15%) CE Loss: 2004.7298, KL Loss: 2985.4146, BLEU score: 0.13, Gaussian score: 0.00\n",
      "11m 48s (- 61m 59s) (16 16%) CE Loss: 2026.3806, KL Loss: 2698.8003, BLEU score: 0.12, Gaussian score: 0.00\n",
      "12m 32s (- 61m 11s) (17 17%) CE Loss: 2046.6151, KL Loss: 2273.5188, BLEU score: 0.10, Gaussian score: 0.00\n",
      "13m 15s (- 60m 25s) (18 18%) CE Loss: 2092.9899, KL Loss: 2028.7091, BLEU score: 0.09, Gaussian score: 0.00\n",
      "13m 59s (- 59m 40s) (19 19%) CE Loss: 2119.3039, KL Loss: 1704.6458, BLEU score: 0.08, Gaussian score: 0.00\n",
      "14m 43s (- 58m 54s) (20 20%) CE Loss: 2152.4639, KL Loss: 1432.2903, BLEU score: 0.15, Gaussian score: 0.00\n",
      "15m 29s (- 58m 15s) (21 21%) CE Loss: 2199.7037, KL Loss: 1113.6039, BLEU score: 0.10, Gaussian score: 0.00\n",
      "16m 14s (- 57m 33s) (22 22%) CE Loss: 2220.9139, KL Loss: 932.5721, BLEU score: 0.06, Gaussian score: 0.00\n",
      "16m 58s (- 56m 49s) (23 23%) CE Loss: 2218.2039, KL Loss: 722.9239, BLEU score: 0.08, Gaussian score: 0.00\n",
      "17m 44s (- 56m 10s) (24 24%) CE Loss: 2196.2920, KL Loss: 674.8597, BLEU score: 0.12, Gaussian score: 0.00\n",
      "18m 31s (- 55m 33s) (25 25%) CE Loss: 2208.1844, KL Loss: 624.1719, BLEU score: 0.07, Gaussian score: 0.00\n",
      "19m 19s (- 55m 0s) (26 26%) CE Loss: 2220.4928, KL Loss: 545.0962, BLEU score: 0.05, Gaussian score: 0.00\n",
      "20m 7s (- 54m 24s) (27 27%) CE Loss: 2231.9802, KL Loss: 498.9660, BLEU score: 0.07, Gaussian score: 0.00\n",
      "20m 55s (- 53m 47s) (28 28%) CE Loss: 2245.3327, KL Loss: 445.1761, BLEU score: 0.07, Gaussian score: 0.00\n",
      "21m 42s (- 53m 10s) (29 28%) CE Loss: 2243.9803, KL Loss: 374.3580, BLEU score: 0.08, Gaussian score: 0.00\n",
      "22m 30s (- 52m 31s) (30 30%) CE Loss: 2213.4733, KL Loss: 369.9156, BLEU score: 0.09, Gaussian score: 0.00\n",
      "23m 18s (- 51m 53s) (31 31%) CE Loss: 2198.3177, KL Loss: 313.5252, BLEU score: 0.11, Gaussian score: 0.00\n",
      "24m 8s (- 51m 18s) (32 32%) CE Loss: 2201.0502, KL Loss: 281.3266, BLEU score: 0.06, Gaussian score: 0.00\n",
      "24m 57s (- 50m 41s) (33 33%) CE Loss: 2221.8274, KL Loss: 261.0437, BLEU score: 0.07, Gaussian score: 0.00\n",
      "25m 46s (- 50m 1s) (34 34%) CE Loss: 2217.9066, KL Loss: 206.7343, BLEU score: 0.06, Gaussian score: 0.00\n",
      "26m 34s (- 49m 21s) (35 35%) CE Loss: 2207.7352, KL Loss: 154.2425, BLEU score: 0.07, Gaussian score: 0.00\n",
      "27m 22s (- 48m 40s) (36 36%) CE Loss: 2204.2939, KL Loss: 153.0592, BLEU score: 0.10, Gaussian score: 0.00\n",
      "28m 10s (- 47m 58s) (37 37%) CE Loss: 2209.2054, KL Loss: 138.3803, BLEU score: 0.08, Gaussian score: 0.00\n",
      "28m 58s (- 47m 16s) (38 38%) CE Loss: 2209.3382, KL Loss: 94.4928, BLEU score: 0.06, Gaussian score: 0.00\n",
      "29m 47s (- 46m 35s) (39 39%) CE Loss: 2196.1089, KL Loss: 65.9109, BLEU score: 0.07, Gaussian score: 0.00\n",
      "30m 35s (- 45m 52s) (40 40%) CE Loss: 2172.2591, KL Loss: 43.9739, BLEU score: 0.06, Gaussian score: 0.00\n",
      "31m 23s (- 45m 11s) (41 41%) CE Loss: 2198.4416, KL Loss: 31.3538, BLEU score: 0.08, Gaussian score: 0.00\n",
      "32m 11s (- 44m 27s) (42 42%) CE Loss: 2199.1554, KL Loss: 22.3860, BLEU score: 0.05, Gaussian score: 0.00\n",
      "32m 59s (- 43m 44s) (43 43%) CE Loss: 2173.0113, KL Loss: 22.7463, BLEU score: 0.06, Gaussian score: 0.00\n",
      "33m 47s (- 43m 0s) (44 44%) CE Loss: 2156.4086, KL Loss: 23.7028, BLEU score: 0.07, Gaussian score: 0.00\n"
     ]
    }
   ],
   "source": [
    "encoder1 = EncoderRNN(vocab_size, hidden_size, laten_size, condition_size).to(device)\n",
    "decoder1 = DecoderRNN(laten_size, hidden_size, vocab_size, condition_size).to(device)\n",
    "vocab = Vocabuary()\n",
    "scores, losses = trainIters(encoder1, decoder1, vocab, 100, print_every=1, plot_every=1, \n",
    "                            batch_size=1, learning_rate=0.01, laten_size=32, condition_size=4, teacher_forcing_ratio=0.7)\n",
    "show_result(scores, losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-05-12T06:21:02.408Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_, _  = evaluate(encoder1, decoder1, vocab, batch_size=32, plot_pred=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-11T03:58:34.577231Z",
     "start_time": "2020-05-11T03:58:34.523642Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
